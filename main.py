import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
import json
import re
import chromadb
import uuid
import random
import os
from datetime import datetime
import google.generativeai as genai
from scenarios import get_system_prompt, get_mission_metadata

# === 1. ì„¤ì • íŒŒì¼ ë¡œë“œ ===
try:
    with open("config.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    print(f"âš™ï¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ: ëª¨ë“œ=[{config['ai_mode']}]")
except FileNotFoundError:
    print("âŒ config.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê¸°ë³¸ê°’(local)ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    config = {"ai_mode": "local", "local_model_name": "mistral", "google_api_key": ""}

# === 2. AI ì´ˆê¸°í™” ===
AI_MODE = config.get("ai_mode", "local").lower()

# [Cloud ì„¤ì •]
if AI_MODE == "cloud":
    api_key = config.get("google_api_key", "")
    if not api_key or "ì—¬ê¸°ì—" in api_key:
        print("âš ï¸ ê²½ê³ : Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. config.jsonì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        genai.configure(api_key=api_key)
        # JSON ëª¨ë“œ ê°•ì œ ì„¤ì • (ë§¤ìš° ì¤‘ìš”)
        gemini_model = genai.GenerativeModel(
            config.get("cloud_model_name", "gemini-1.5-flash"),
            generation_config={"response_mime_type": "application/json"}
        )
        print("â˜ï¸ Cloud AI (Gemini) ëª¨ë“œë¡œ ëŒ€ê¸° ì¤‘...")

# [Local ì„¤ì •]
else:
    OLLAMA_URL = "http://localhost:11434/api/chat"
    LOCAL_MODEL = config.get("local_model_name", "mistral")
    print(f"ğŸ  Local AI ({LOCAL_MODEL}) ëª¨ë“œë¡œ ëŒ€ê¸° ì¤‘... (Ollama ì¼œì ¸ ìˆë‚˜ìš”?)")


# === 3. DB ë° ì•± ì„¤ì • ===
try:
    chroma_client = chromadb.PersistentClient(path="./memory_db")
    collection = chroma_client.get_or_create_collection(name="game_memory")
except Exception:
    collection = None

app = FastAPI(title="Social Engineer Backend")

class GameRequest(BaseModel):
    player_input: str
    suspicion: int = 0
    scenario_id: str = "mission_1"

class GameResponse(BaseModel):
    dialogue: str
    suspicion_delta: int = 0
    action: str = "NONE"

# === ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ===
def add_memory(text, speaker):
    if collection:
        collection.add(
            documents=[text],
            metadatas=[{"speaker": speaker, "timestamp": str(datetime.now())}],
            ids=[str(uuid.uuid4())]
        )

def retrieve_memory(query, n_results=3):
    if not collection: return ""
    try:
        results = collection.query(query_texts=[query], n_results=n_results)
        if not results['documents']: return ""
        return "\n".join([f"- {m}" for m in results['documents'][0]])
    except Exception:
        return ""

@app.get("/mission/{scenario_id}")
async def get_mission_info(scenario_id: str):
    metadata = get_mission_metadata(scenario_id)
    docs = metadata.get("secret_documents", [])
    selected_secret = random.choice(docs) if docs else "ê¸°ë°€ ë¬¸ì„œ ì—†ìŒ"
    response_data = metadata.copy()
    response_data["target_secret"] = selected_secret
    if "secret_documents" in response_data: del response_data["secret_documents"]
    return response_data

# === 4. í•˜ì´ë¸Œë¦¬ë“œ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ===
@app.post("/chat", response_model=GameResponse)
async def chat_endpoint(request: GameRequest):
    print(f"ğŸ“© Input: {request.player_input} (Mode: {AI_MODE})")

    try:
        memories = retrieve_memory(request.player_input)
        system_instruction = get_system_prompt(request.scenario_id, memories)
        
        # --- [A] CLOUD MODE (Gemini) ---
        if AI_MODE == "cloud":
            chat = gemini_model.start_chat(history=[
                {"role": "user", "parts": [f"System:\n{system_instruction}"]}
            ])
            response = await chat.send_message_async(request.player_input)
            raw_content = response.text
            print(f"â˜ï¸ Gemini ì‘ë‹µ: {raw_content}")

        # --- [B] LOCAL MODE (Ollama) ---
        else:
            messages = [
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": request.player_input}
            ]
            payload = {
                "model": LOCAL_MODEL,
                "messages": messages,
                "stream": False,
                "format": "json",
                "options": {"temperature": 0.7}
            }
            async with httpx.AsyncClient() as client:
                resp = await client.post(OLLAMA_URL, json=payload, timeout=45.0)
                resp.raise_for_status()
                raw_content = resp.json().get("message", {}).get("content", "")
                print(f"ğŸ  Local ì‘ë‹µ: {raw_content}")

        # --- ê³µí†µ ì²˜ë¦¬ (JSON íŒŒì‹± ë° ì €ì¥) ---
        add_memory(f"User: {request.player_input}", "player")
        
        try:
            ai_json = json.loads(raw_content)
            dialogue = ai_json.get("dialogue", "...")
            add_memory(f"NPC: {dialogue}", "npc")
            
            # íŠ¹ìˆ˜ë¬¸ì ì²­ì†Œ (ì„ íƒ ì‚¬í•­)
            dialogue = re.sub(r"[^\uAC00-\uD7A30-9a-zA-Z\s.,?!'\"~()]", "", dialogue)

            return GameResponse(
                dialogue=dialogue,
                suspicion_delta=ai_json.get("suspicion_delta", 0),
                action=ai_json.get("action", "NONE")
            )
        except json.JSONDecodeError:
            print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜")
            return GameResponse(dialogue=raw_content, suspicion_delta=0)

    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        error_msg = "[ì¸í„°ë„· ì—°ê²° ë¶ˆì•ˆì •]" if AI_MODE == "cloud" else "[AI ì„œë²„ ì‘ë‹µ ì—†ìŒ]"
        return GameResponse(dialogue=error_msg, suspicion_delta=0)

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)