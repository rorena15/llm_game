import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
import json
import re
import chromadb
import uuid
import random
import os
from datetime import datetime
from typing import Optional
import google.generativeai as genai
from scenarios import get_system_prompt, get_mission_metadata

# === ìƒìˆ˜ ì •ì˜ ===
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/chat")
REQUEST_TIMEOUT = 45.0
MEMORY_RETRIEVE_LIMIT = 3
DB_PATH = "./memory_db"
MAX_DIALOGUE_LENGTH = 100

# === 1. ì„¤ì • íŒŒì¼ ë¡œë“œ ===
def load_config() -> dict:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê²€ì¦"""
    try:
        with open("config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
        print(f"âš™ï¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ: ëª¨ë“œ=[{config['ai_mode']}]")
        return config
    except FileNotFoundError:
        print("âŒ config.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê¸°ë³¸ê°’(local)ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return {
            "ai_mode": "local",
            "local_model_name": "mistral",
            "google_api_key": ""
        }
    except json.JSONDecodeError as e:
        print(f"âŒ config.json íŒŒì‹± ì˜¤ë¥˜: {e}")
        raise

config = load_config()

# === 2. AI ì´ˆê¸°í™” ===
AI_MODE = config.get("ai_mode", "local").lower()
gemini_model = None

def init_gemini() -> Optional[genai.GenerativeModel]:
    """Gemini AI ì´ˆê¸°í™”"""
    # í™˜ê²½ ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ config ì‚¬ìš©
    api_key = os.getenv("GOOGLE_API_KEY") or config.get("google_api_key", "")
    
    if not api_key or "ì—¬ê¸°ì—" in api_key:
        print("âš ï¸ ê²½ê³ : Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   í™˜ê²½ ë³€ìˆ˜ GOOGLE_API_KEY ë˜ëŠ” config.jsonì„ í™•ì¸í•˜ì„¸ìš”.")
        return None
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(
            config.get("cloud_model_name", "gemini-1.5-flash"),
            generation_config={"response_mime_type": "application/json"}
        )
        print("â˜ï¸ Cloud AI (Gemini) ëª¨ë“œë¡œ ëŒ€ê¸° ì¤‘...")
        return model
    except Exception as e:
        print(f"âŒ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

if AI_MODE == "cloud":
    gemini_model = init_gemini()
else:
    LOCAL_MODEL = config.get("local_model_name", "mistral")
    print(f"ğŸ  Local AI ({LOCAL_MODEL}) ëª¨ë“œë¡œ ëŒ€ê¸° ì¤‘... (Ollama ì¼œì ¸ ìˆë‚˜ìš”?)")

# === 3. DB ì´ˆê¸°í™” ===
def init_chromadb() -> Optional[chromadb.Collection]:
    """ChromaDB ì´ˆê¸°í™”"""
    try:
        chroma_client = chromadb.PersistentClient(path=DB_PATH)
        collection = chroma_client.get_or_create_collection(name="game_memory")
        print(f"ğŸ’¾ ChromaDB ì´ˆê¸°í™” ì™„ë£Œ (ê²½ë¡œ: {DB_PATH})")
        return collection
    except Exception as e:
        print(f"âš ï¸ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("   ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ì—†ì´ ê³„ì†í•©ë‹ˆë‹¤.")
        return None

collection = init_chromadb()

# === 4. FastAPI ì•± ì„¤ì • ===
app = FastAPI(
    title="Social Engineer Backend",
    description="í•˜ì´ë¸Œë¦¬ë“œ AI ê¸°ë°˜ ì†Œì…œ ì—”ì§€ë‹ˆì–´ë§ ê²Œì„",
    version="1.0.0"
)

class GameRequest(BaseModel):
    player_input: str
    suspicion: int = 0
    scenario_id: str = "mission_1"
    session_id: Optional[str] = None  # ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬ ë¶„ë¦¬

class GameResponse(BaseModel):
    dialogue: str
    suspicion_delta: int = 0
    action: str = "NONE"
    error: Optional[str] = None

# === 5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ===
def add_memory(text: str, speaker: str, session_id: Optional[str] = None) -> None:
    """ëŒ€í™” ë©”ëª¨ë¦¬ì— ì¶”ê°€"""
    if not collection:
        return
    
    try:
        metadata = {
            "speaker": speaker,
            "timestamp": str(datetime.now()),
            "session_id": session_id or "default"
        }
        collection.add(
            documents=[text],
            metadatas=[metadata],
            ids=[str(uuid.uuid4())]
        )
    except Exception as e:
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")

def retrieve_memory(query: str, session_id: Optional[str] = None, n_results: int = MEMORY_RETRIEVE_LIMIT) -> str:
    """ê´€ë ¨ ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
    if not collection:
        return ""
    
    try:
        # ì„¸ì…˜ë³„ í•„í„°ë§
        where_filter = {"session_id": session_id or "default"} if session_id else None
        
        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            where=where_filter
        )
        
        if not results['documents'] or not results['documents'][0]:
            return ""
        
        return "\n".join([f"- {doc}" for doc in results['documents'][0]])
    except Exception as e:
        print(f"âš ï¸ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return ""

def sanitize_dialogue(text: str) -> str:
    """ëŒ€í™” í…ìŠ¤íŠ¸ ì •ì œ"""
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¹€)
    cleaned = re.sub(r"[^\uAC00-\uD7A30-9a-zA-Z\s.,?!'\"~()-]", "", text)
    return cleaned.strip()

# === 6. API ì—”ë“œí¬ì¸íŠ¸ ===
@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "online",
        "ai_mode": AI_MODE,
        "memory_enabled": collection is not None,
        "model": LOCAL_MODEL if AI_MODE == "local" else config.get("cloud_model_name")
    }

@app.get("/mission/{scenario_id}")
async def get_mission_info(scenario_id: str):
    """ë¯¸ì…˜ ì •ë³´ ì¡°íšŒ"""
    try:
        metadata = get_mission_metadata(scenario_id)
        docs = metadata.get("secret_documents", [])
        selected_secret = random.choice(docs) if docs else "ê¸°ë°€ ë¬¸ì„œ ì—†ìŒ"
        
        response_data = metadata.copy()
        response_data["target_secret"] = selected_secret
        if "secret_documents" in response_data:
            del response_data["secret_documents"]
        
        return response_data
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scenario_id}")

async def call_gemini(system_instruction: str, user_input: str) -> str:
    """Gemini API í˜¸ì¶œ"""
    if not gemini_model:
        raise HTTPException(status_code=503, detail="Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    chat = gemini_model.start_chat(history=[
        {"role": "user", "parts": [f"System:\n{system_instruction}"]}
    ])
    response = await chat.send_message_async(user_input)
    return response.text

async def call_ollama(system_instruction: str, user_input: str) -> str:
    """Ollama API í˜¸ì¶œ"""
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": user_input}
    ]
    
    payload = {
        "model": LOCAL_MODEL,
        "messages": messages,
        "stream": False,
        "format": "json",
        "options": {"temperature": 0.7}
    }
    
    async with httpx.AsyncClient() as client:
        resp = await client.post(OLLAMA_URL, json=payload, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        return resp.json().get("message", {}).get("content", "")

@app.post("/chat", response_model=GameResponse)
async def chat_endpoint(request: GameRequest):
    """ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    print(f"ğŸ“© Input: {request.player_input} (Mode: {AI_MODE}, Session: {request.session_id})")

    try:
        # ë©”ëª¨ë¦¬ ê²€ìƒ‰
        memories = retrieve_memory(request.player_input, request.session_id)
        system_instruction = get_system_prompt(request.scenario_id, memories)
        
        # --- AI í˜¸ì¶œ ---
        if AI_MODE == "cloud":
            raw_content = await call_gemini(system_instruction, request.player_input)
            print(f"â˜ï¸ Gemini ì‘ë‹µ: {raw_content[:100]}...")
        else:
            raw_content = await call_ollama(system_instruction, request.player_input)
            print(f"ğŸ  Ollama ì‘ë‹µ: {raw_content[:100]}...")

        # === [ìˆ˜ì •ëœ íŒŒì‹± ë° í›„ì²˜ë¦¬ ë¡œì§] ===
        add_memory(f"User: {request.player_input}", "player", request.session_id)
        
        dialogue = "..."
        suspicion_delta = 0
        action = "NONE"
        
        try:
            # 1. 1ì°¨ ì‹œë„: ìˆœìˆ˜ JSON íŒŒì‹±
            ai_json = json.loads(raw_content)
            dialogue = ai_json.get("dialogue", "...")
            suspicion_delta = ai_json.get("suspicion_delta", 0)
            action = ai_json.get("action", "NONE")
            print("âœ… [íŒŒì‹± ì„±ê³µ] ìˆœìˆ˜ JSON íŒŒì‹± ì™„ë£Œ")
            
        except json.JSONDecodeError:
            print(f"âš ï¸ [íŒŒì‹± ì‹¤íŒ¨] 1ì°¨ JSON ì‹¤íŒ¨. ìë™ ë³µêµ¬ ì‹œë„...")
            
            # 2. 2ì°¨ ì‹œë„: ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```json ... ```) ì¶”ì¶œ
            json_match = re.search(r"```(?:json)?\s*({.*?})\s*```", raw_content, re.DOTALL)
            parsing_success = False
            
            if json_match:
                try:
                    ai_json = json.loads(json_match.group(1))
                    dialogue = ai_json.get("dialogue", "...")
                    suspicion_delta = ai_json.get("suspicion_delta", 0)
                    action = ai_json.get("action", "NONE")
                    parsing_success = True
                    print("âœ… [ë³µêµ¬ ì„±ê³µ] ë§ˆí¬ë‹¤ìš´ì—ì„œ JSON ì¶”ì¶œ ì™„ë£Œ")
                except:
                    pass  # ì¶”ì¶œí–ˆëŠ”ë°ë„ ê¹¨ì ¸ìˆìœ¼ë©´ íŒ¨ìŠ¤
            
            # 3. ìµœí›„ì˜ ë³´ë£¨: í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì¶œë ¥ (ê²Œì„ì  í—ˆìš©)
            if not parsing_success:
                print("âŒ [ë³µêµ¬ ì‹¤íŒ¨] ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš© ë° íŒ¨ë„í‹° ë¶€ì—¬")
                dialogue = raw_content.strip()
                
                # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (UI ë³´í˜¸)
                if len(dialogue) > MAX_DIALOGUE_LENGTH:
                    dialogue = dialogue[:97] + "..."
                
                # íŒ¨ë„í‹°: AIê°€ í¬ë§·ì„ ì–´ê²¼ìœ¼ë¯€ë¡œ ì˜ì‹¬ë„ ëŒ€í­ ì¦ê°€
                suspicion_delta = 20
                action = "GLITCH"  # í˜¹ì€ NONE
        
        # === ê³µí†µ í›„ì²˜ë¦¬ (ë©”ëª¨ ì €ì¥ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°) ===
        # 1. íŠ¹ìˆ˜ë¬¸ì ì²­ì†Œ (í•œêµ­ì–´, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ í—ˆìš©)
        # íŠœë‹: ëŒ€ê´„í˜¸[]ë‚˜ ì¤‘ê´„í˜¸{}ê°€ ê·¸ëŒ€ë¡œ ë…¸ì¶œë˜ëŠ”ê±¸ ë§‰ìœ¼ë ¤ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
        dialogue = sanitize_dialogue(dialogue)
        
        # 2. NPC ê¸°ì–µ ì €ì¥
        add_memory(f"NPC: {dialogue}", "npc", request.session_id)
        
        # 3. ìµœì¢… ì‘ë‹µ ë°˜í™˜
        return GameResponse(
            dialogue=dialogue,
            suspicion_delta=suspicion_delta,
            action=action
        )

    except httpx.TimeoutException:
        error_msg = "[ì‘ë‹µ ì‹œê°„ ì´ˆê³¼] AI ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        return GameResponse(dialogue=error_msg, suspicion_delta=0, error="timeout")
    
    except httpx.HTTPStatusError as e:
        error_msg = f"[ì—°ê²° ì˜¤ë¥˜] ìƒíƒœ ì½”ë“œ: {e.response.status_code}"
        return GameResponse(dialogue=error_msg, suspicion_delta=0, error="http_error")
    
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
        error_msg = "[ì‹œìŠ¤í…œ ì˜¤ë¥˜] ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        return GameResponse(dialogue=error_msg, suspicion_delta=0, error=str(e))

# === 7. ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ ===
@app.delete("/memory/{session_id}")
async def clear_session_memory(session_id: str):
    """íŠ¹ì • ì„¸ì…˜ì˜ ë©”ëª¨ë¦¬ ì‚­ì œ"""
    if not collection:
        raise HTTPException(status_code=503, detail="ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
    
    try:
        # í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
        results = collection.get(where={"session_id": session_id})
        if results['ids']:
            collection.delete(ids=results['ids'])
            return {"message": f"{len(results['ids'])}ê°œì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤", "session_id": session_id}
        return {"message": "ì‚­ì œí•  ë©”ëª¨ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤", "session_id": session_id}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”ëª¨ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)